---
title: "Week 11 Assignment"
author: "Amisha Meka"
date: "2025-05-01"
output: html_document
---

```{r}
library(mlbench)
library(purrr)

data("PimaIndiansDiabetes2")
ds <- as.data.frame(na.omit(PimaIndiansDiabetes2))
## fit a logistic regression model to obtain a parametric equation
logmodel <- glm(diabetes ~ .,
                data = ds,
                family = "binomial")
summary(logmodel)

cfs <- coefficients(logmodel) ## extract the coefficients
prednames <- variable.names(ds)[-9] ## fetch the names of predictors in a vector
prednames

sz <- 100000000 ## to be used in sampling
##sample(ds$pregnant, size = sz, replace = T)

dfdata <- map_dfc(prednames,
                  function(nm){ ## function to create a sample-with-replacement for each pred.
                    eval(parse(text = paste0("sample(ds$",nm,
                                             ", size = sz, replace = T)")))
                  }) ## map the sample-generator on to the vector of predictors
## and combine them into a dataframe

names(dfdata) <- prednames
dfdata

class(cfs[2:length(cfs)])

length(cfs)
length(prednames)
## Next, compute the logit values
pvec <- map((1:8),
            function(pnum){
              cfs[pnum+1] * eval(parse(text = paste0("dfdata$",
                                                     prednames[pnum])))
            }) %>% ## create beta[i] * x[i]
  reduce(`+`) + ## sum(beta[i] * x[i])
  cfs[1] ## add the intercept

## exponentiate the logit to obtain probability values of thee outcome variable
dfdata$outcome <- ifelse(1/(1 + exp(-(pvec))) > 0.5,
                         1, 0)
```


```{r}
##XGBoost Direct:
library(xgboost)
library(dplyr)

set.seed(123) # For reproducibility

# Make sure outcome is numeric
dfdata$outcome <- as.numeric(dfdata$outcome)

# Define dataset sizes
sizes <- c(100, 1000, 10000, 100000, 1000000, 10000000)

# Initialize result storage
results <- data.frame(
  Dataset_Size = integer(),
  Accuracy = numeric(),
  Time_Taken_Sec = numeric()
)

for (sz in sizes) {
  
  # Sample the data
  idx <- sample(1:nrow(dfdata), sz)
  tempdata <- dfdata[idx, ]
  
  # Split into training and testing sets (80/20 split)
  train_idx <- sample(1:nrow(tempdata), 0.8 * nrow(tempdata))
  train <- tempdata[train_idx, ]
  test <- tempdata[-train_idx, ]
  
  # Prepare data matrices for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(train[, -which(names(train) == "outcome")]), label = train$outcome)
  dtest <- xgb.DMatrix(data = as.matrix(test[, -which(names(test) == "outcome")]), label = test$outcome)
  
  # Train XGBoost and measure time
  start_time <- Sys.time()
  
  model <- xgboost(
    data = dtrain,
    objective = "binary:logistic",
    nrounds = 50,
    verbose = 0
  )
  
  end_time <- Sys.time()
  
  # Predict and calculate accuracy
  preds <- predict(model, dtest)
  pred_labels <- ifelse(preds > 0.5, 1, 0)
  accuracy <- mean(pred_labels == test$outcome)
  
  # Save the results
  results <- rbind(results, data.frame(
    Dataset_Size = sz,
    Accuracy = round(accuracy, 4),
    Time_Taken_Sec = round(as.numeric(difftime(end_time, start_time, units = "secs")), 2)
  ))
}

# View the results
results
```


```{r}
##XGBoost with caret
library(caret)
library(xgboost)
library(dplyr)

set.seed(123)

# Fix the outcome labels
dfdata$outcome <- ifelse(dfdata$outcome == 1, "Class1", "Class0")
dfdata$outcome <- as.factor(dfdata$outcome)

# Define dataset sizes
sizes <- c(100, 1000, 10000, 100000, 1000000, 10000000)

# Initialize result storage
results_caret <- data.frame(
  Dataset_Size = integer(),
  Accuracy = numeric(),
  Time_Taken_Sec = numeric()
)

for (sz in sizes) {
  
  # Sample the data
  idx <- sample(1:nrow(dfdata), sz)
  tempdata <- dfdata[idx, ]
  
  # Create 80/20 split
  train_idx <- sample(1:nrow(tempdata), 0.8 * nrow(tempdata))
  train <- tempdata[train_idx, ]
  test <- tempdata[-train_idx, ]
  
  # Set up 5-fold cross-validation
  train_control <- trainControl(
    method = "cv",
    number = 5,
    verboseIter = FALSE,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
  
  x_train <- train[, -which(names(train) == "outcome")]
  y_train <- train$outcome
  
  start_time <- Sys.time()
  
  model <- train(
    x = x_train,
    y = y_train,
    method = "xgbTree",
    trControl = train_control,
    metric = "Accuracy",
    tuneLength = 1
  )
  
  end_time <- Sys.time()
  
  # Predict on test set
  preds <- predict(model, test[, -which(names(test) == "outcome")])
  
  # Calculate accuracy
  accuracy <- mean(preds == test$outcome)
  
  # Save results
  results_caret <- rbind(results_caret, data.frame(
    Dataset_Size = sz,
    Accuracy = round(accuracy, 4),
    Time_Taken_Sec = round(as.numeric(difftime(end_time, start_time, units = "secs")), 2)
  ))
}

# View results
results_caret
```

